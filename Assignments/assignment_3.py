# -*- coding: utf-8 -*-
"""assignment 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wrbU49jL77p3qlhe1LqUQ5E8tgePu3XZ

#1
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split

dataset = load_diabetes()
x_data = dataset.data
y_data = dataset.target
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)
x_train

class LinearRegression:
    def __init__(self, learning_rate=0.01):
        self.w = None # weight
        self.b = None # bias
        self.learn = learning_rate # learning_rate
        self.losses = [] # losses for each epoch
        self.weight_history = [] # weight for each epoch
        self.bias_history = [] # bias for each epoch

    def forward(self, x):
        y_pred = np.sum(x * self.w) + self.b # hypothesis function
        return y_pred

    def loss(self, x, y):
        y_pred = self.forward(x)
        return (y_pred-y) ** 2 # cost function

    def gradient(self, x, y):
        y_pred = self.forward(x)
        w_grad = 2 * x * (y_pred-y) # weight gradient
        b_grad = 2 * (y_pred-y) # bias gradient

        return w_grad, b_grad

    def fit(self, x_data, y_data, epochs=1000):
        self.w = np.ones(x_data.shape[1]) # initialize weights to 1
        self.b = 0 # initialize bias to 0
        for epoch in range(epochs):
            l = 0 # initial value of loss
            w_grad = np.zeros(x_data.shape[1]) # array for gradient of weight
            b_grad = 0  # variable for gradient of bias

            for x,y in zip(x_data, y_data):
                # compute loss(cost) function in initial value
                l += self.loss(x, y)

                # compute gradient value in initial value
                w_i, b_i = self.gradient(x, y)
                w_grad += w_i
                b_grad += b_i

            self.w -= self.learn * (w_grad/len(y_data)) # update weight 
            self.b -= self.learn * (b_grad/len(y_data)) # update bias 

            print(f'epoch : {epoch+1} | loss : {l/len(y_data):.3f}')

            self.losses.append(l/len(y_data)) # save loss
            self.weight_history.append(self.w) # save weight
            self.bias_history.append(self.b) # save bias
            
model = LinearRegression()
model.fit(x_train, y_train)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDRegressor

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)

std_scale = StandardScaler()
std_scale.fit(x_train)

x_train_std = std_scale.transform(x_train)
x_test_std = std_scale.transform(x_test)

sgd = SGDRegressor(max_iter=1000)
sgd.fit(x_train_std, y_train)
print(sgd.coef_, sgd.intercept_)

"""#2"""

from sklearn.datasets import make_regression
import matplotlib.pyplot as plt
x, y, c = make_regression(n_samples=5000, n_features=2, bias=10, noise=10, coef=True, random_state=42)
plt.scatter(x[:,0], x[:,1], c=y, s=5, alpha=0.5)
plt.xlabel("x1")
plt.ylabel("x2")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

std_scale = StandardScaler()
std_scale.fit(x_train)

x_train_std = std_scale.transform(x_train)
x_test_std = std_scale.transform(x_test)

lr = LinearRegression()
lr.fit(x_train_std, y_train)
print(lr.coef_, lr.intercept_)

"""#3"""

from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
x, y = make_classification(n_samples=1000, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1, random_state=42)
plt.scatter(x, y, c=y, s=5)
plt.xlabel("x")
plt.ylabel("y")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

lr = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)
lr.fit(x_train, y_train)